"""
LLM Service Î³Î¹Î± ÎµÏ€Î¹ÎºÎ¿Î¹Î½Ï‰Î½Î¯Î± Î¼Îµ Ollama/Mistral.

Î‘Ï…Ï„ÏŒ Ï„Î¿ module ÎµÎ¯Î½Î±Î¹ Ï…Ï€ÎµÏÎ¸Ï…Î½Î¿ Î³Î¹Î±:
1. Î¤Î·Î½ ÎµÏ€Î¹ÎºÎ¿Î¹Î½Ï‰Î½Î¯Î± Î¼Îµ Ï„Î¿ Ollama API
2. Î¤Î¿ prompt engineering
3. Î¤Î· Î´Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ· Ï„Î¿Ï… context
"""

import json
import requests
from typing import Dict, Any
import logging

# Î¡Ï…Î¸Î¼Î¯Î¶Î¿Ï…Î¼Îµ Ï„Î¿ logging Î³Î¹Î± debugging
logger = logging.getLogger(__name__)

# Configuration
OLLAMA_BASE_URL = "http://localhost:11434"
DEFAULT_MODEL = "mistral"
DEFAULT_TEMPERATURE = 0.7  # Î ÏŒÏƒÎ¿ "Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¹ÎºÏŒ" Î¸Î± ÎµÎ¯Î½Î±Î¹ Ï„Î¿ model (0-1)
DEFAULT_MAX_TOKENS = 500   # ÎœÎ­Î³Î¹ÏƒÏ„Î¿ Î¼Î®ÎºÎ¿Ï‚ Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·Ï‚


class LLMService:
    """
    Service class Î³Î¹Î± Ï„Î·Î½ ÎµÏ€Î¹ÎºÎ¿Î¹Î½Ï‰Î½Î¯Î± Î¼Îµ Ï„Î¿ Ollama API.
    
    Î‘Ï…Ï„Î® Î· ÎºÎ»Î¬ÏƒÎ· ÎºÎ¬Î½ÎµÎ¹ abstract Ï„Î·Î½ ÎµÏ€Î¹ÎºÎ¿Î¹Î½Ï‰Î½Î¯Î± Î¼Îµ Ï„Î¿ LLM,
    ÏÏƒÏ„Îµ Î¿ Ï…Ï€ÏŒÎ»Î¿Î¹Ï€Î¿Ï‚ ÎºÏÎ´Î¹ÎºÎ±Ï‚ Î½Î± Î¼Î·Î½ Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹ Î½Î± Î¾Î­ÏÎµÎ¹
    Ï„Î¹Ï‚ Î»ÎµÏ€Ï„Î¿Î¼Î­ÏÎµÎ¹ÎµÏ‚ Ï„Î¿Ï… Ï€ÏÏ‚ Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³ÎµÎ¯ Ï„Î¿ Ollama.
    """
    
    def __init__(
        self, 
        base_url: str = OLLAMA_BASE_URL,
        model: str = DEFAULT_MODEL,
        temperature: float = DEFAULT_TEMPERATURE
    ):
        """
        Initialize the LLM service.
        
        Args:
            base_url: Î¤Î¿ URL ÏŒÏ€Î¿Ï… Ï„ÏÎ­Ï‡ÎµÎ¹ Ï„Î¿ Ollama
            model: Î¤Î¿ model Ï€Î¿Ï… Î¸Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÎ¿Ï…Î¼Îµ
            temperature: Î ÏŒÏƒÎ¿ "Ï„Ï…Ï‡Î±Î¯ÎµÏ‚" Î¸Î± ÎµÎ¯Î½Î±Î¹ Î¿Î¹ Î±Ï€Î±Î½Ï„Î®ÏƒÎµÎ¹Ï‚
        """
        self.base_url = base_url
        self.model = model
        self.temperature = temperature
        self._check_connection()

    def _check_connection(self) -> None:
        """Check if we can connect to Ollama."""
        try:
            response = requests.get(f"{self.base_url}/api/tags")
            response.raise_for_status()
            logger.info(f"âœ… Connected to Ollama at {self.base_url}")
        except Exception as e:
            logger.error(f"âŒ Cannot connect to Ollama: {e}")
            logger.error("Make sure Ollama is running with: ollama serve")
            raise ConnectionError(
                f"Cannot connect to Ollama at {self.base_url}"
            )

    def create_system_prompt(self, knowledge_base: str) -> str:
        """Create the system prompt for the AI."""
        return (
            "You are a helpful customer support assistant for CloudSphere Platform.\n\n"
            "Your role is to answer customer questions based ONLY on the following knowledge base.\n"
            "If a question cannot be answered using the knowledge base, politely say that you don't have that information.\n\n"
            "IMPORTANT RULES:\n"
            "1. Only use information from the knowledge base below\n"
            "2. Be concise and direct in your answers\n"
            "3. If you're not sure, say so - don't make up information\n"
            "4. Be friendly and professional\n"
            "5. Format your answers clearly with proper punctuation\n"
            "6. If the question is about something not in the knowledge base, respond with: \"I don't have information about that in my knowledge base.\"\n"
            "   Please contact support@cloudsphere.com for assistance with topics not covered here.\n"
            "7. If someone asks a general greeting (hello, hi, etc.), respond professionally but remind them you're here to answer CloudSphere-related questions\n\n"
            "KNOWLEDGE BASE:\n"
            f"{knowledge_base}\n\n"
            "Remember: You are ONLY a CloudSphere support assistant. Do not answer questions unrelated to CloudSphere or provide information not in the knowledge base above."
        )
    
    def generate_answer(
        self,
        question: str,
        knowledge_base: str,
        max_tokens: int = DEFAULT_MAX_TOKENS
    ) -> str:
        """
        Î£Ï„Î­Î»Î½ÎµÎ¹ ÎµÏÏÏ„Î·ÏƒÎ· ÏƒÏ„Î¿ LLM ÎºÎ±Î¹ Ï€Î±Î¯ÏÎ½ÎµÎ¹ Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·.
        
        Î— Î´Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¯Î±:
        1. Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¿ÏÎ¼Îµ Ï„Î¿ system prompt Î¼Îµ Ï„Î¿ knowledge base
        2. Î£Ï…Î½Î´Ï…Î¬Î¶Î¿Ï…Î¼Îµ Î¼Îµ Ï„Î·Î½ ÎµÏÏÏ„Î·ÏƒÎ· Ï„Î¿Ï… Ï‡ÏÎ®ÏƒÏ„Î·
        3. Î£Ï„Î­Î»Î½Î¿Ï…Î¼Îµ ÏƒÏ„Î¿ Ollama
        4. Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†Î¿Ï…Î¼Îµ Ï„Î·Î½ Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·
        
        Args:
            question: Î— ÎµÏÏÏ„Î·ÏƒÎ· Ï„Î¿Ï… Ï‡ÏÎ®ÏƒÏ„Î·
            knowledge_base: Î¤Î¿ Ï€ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î¿ Ï€Î¿Ï… Î¸Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÎµÎ¹ Î³Î¹Î± context
            max_tokens: ÎœÎ­Î³Î¹ÏƒÏ„Î¿ Î¼Î®ÎºÎ¿Ï‚ Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·Ï‚
            
        Returns:
            Î— Î±Ï€Î¬Î½Ï„Î·ÏƒÎ· Î±Ï€ÏŒ Ï„Î¿ LLM
        """
        try:
            # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¿ÏÎ¼Îµ Ï„Î¿ Ï€Î»Î®ÏÎµÏ‚ prompt
            system_prompt = self.create_system_prompt(knowledge_base)
            full_prompt = (
                f"{system_prompt}\n\n"
                f"Customer Question: {question}\n\n"
                "Assistant Answer:"
            )
            
            # Î ÏÎ¿ÎµÏ„Î¿Î¹Î¼Î¬Î¶Î¿Ï…Î¼Îµ Ï„Î¿ request Î³Î¹Î± Ï„Î¿ Ollama
            payload: Dict[str, Any] = {
                "model": self.model,
                "prompt": full_prompt,
                "temperature": self.temperature,
                "stream": False,
                "options": {
                    "num_predict": max_tokens,
                    "stop": [
                        "Customer Question:",
                        "\n\n"
                    ]
                }
            }
            
            # Log Î³Î¹Î± debugging
            logger.info(f"ğŸ“¤ Sending question to LLM: {question[:50]}...")
            
            # Î£Ï„Î­Î»Î½Î¿Ï…Î¼Îµ Ï„Î¿ request
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=payload,
                timeout=30  # 30 seconds timeout
            )
            response.raise_for_status()
            
            # Î Î±Î¯ÏÎ½Î¿Ï…Î¼Îµ Ï„Î·Î½ Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·
            result = response.json()
            answer = result.get("response", "").strip()
            
            # Î‘Î½ Î· Î±Ï€Î¬Î½Ï„Î·ÏƒÎ· ÎµÎ¯Î½Î±Î¹ ÎºÎµÎ½Î®, ÎºÎ¬Ï„Î¹ Ï€Î®Î³Îµ ÏƒÏ„ÏÎ±Î²Î¬
            if not answer:
                logger.error("Empty response from LLM")
                return (
                    "I apologize, but I couldn't generate an answer. "
                    "Please try again."
                )
            
            logger.info(f"âœ… Received answer from LLM ({len(answer)} chars)")
            return answer
            
        except requests.exceptions.Timeout:
            logger.error("LLM request timed out")
            return (
                "The request took too long. "
                "Please try again with a simpler question."
            )
            
        except requests.exceptions.RequestException as e:
            logger.error(f"Request error: {e}")
            return (
                "I'm having trouble connecting to the AI service. "
                "Please try again later."
            )
            
        except Exception as e:
            logger.error(f"Unexpected error: {e}")
            return "An unexpected error occurred. Please try again."
    
    def test_connection(self) -> Dict[str, Any]:
        """
        Î¤ÎµÏƒÏ„Î¬ÏÎµÎ¹ Ï„Î· ÏƒÏÎ½Î´ÎµÏƒÎ· ÎºÎ±Î¹ ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚.
        
        Î§ÏÎ®ÏƒÎ¹Î¼Î¿ Î³Î¹Î± debugging ÎºÎ±Î¹ health checks.
        """
        try:
            # ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ models
            response = requests.get(f"{self.base_url}/api/tags")
            response.raise_for_status()
            models = response.json()
            
            # Î‘Ï€Î»ÏŒ test generation
            test_response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": "Say 'Hello, I am working!'",
                    "stream": False
                }
            )
            test_response.raise_for_status()
            
            return {
                "status": "connected",
                "ollama_url": self.base_url,
                "model": self.model,
                "available_models": [
                    m["name"] for m in models.get("models", [])
                ],
                "test_response": test_response.json().get(
                    "response",
                    "No response"
                )
            }
        except requests.exceptions.RequestException as e:
            return {
                "status": "error",
                "error": str(e),
                "ollama_url": self.base_url
            }


# Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¿ÏÎ¼Îµ Î­Î½Î± global instance Ï„Î¿Ï… service
# Î‘Ï…Ï„ÏŒ Î¸Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î·Î¸ÎµÎ¯ Î±Ï€ÏŒ Ï„Î± routes
llm_service = LLMService()