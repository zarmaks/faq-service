"""
Test script Î³Î¹Î± Ï„Î¿ LLM integration.

Î¤ÏÎ­Î¾Îµ Ï„Î¿ Î¼Îµ: python test_llm.py
"""

import requests
import json
import time

BASE_URL = "http://127.0.0.1:8001"

def test_llm_connection():
    """Î¤ÎµÏƒÏ„Î¬ÏÎµÎ¹ Î±Î½ Ï„Î¿ LLM ÎµÎ¯Î½Î±Î¹ ÏƒÏ…Î½Î´ÎµÎ´ÎµÎ¼Î­Î½Î¿."""
    print("ğŸ” Testing LLM connection...")
    
    response = requests.get(f"{BASE_URL}/api/v1/llm/test")
    
    if response.status_code == 200:
        data = response.json()
        print("âœ… LLM Connection Status:", data.get("status"))
        print("ğŸ“¦ Model:", data.get("model"))
        print("ğŸ–¥ï¸  Ollama URL:", data.get("ollama_url"))
        print("ğŸ“‹ Available Models:", data.get("available_models"))
        return True
    else:
        print("âŒ LLM connection failed:", response.text)
        return False

def test_faq_questions():
    """Î”Î¿ÎºÎ¹Î¼Î¬Î¶ÎµÎ¹ Î´Î¹Î¬Ï†Î¿ÏÎµÏ‚ ÎµÏÏ‰Ï„Î®ÏƒÎµÎ¹Ï‚ ÏƒÏ„Î¿ FAQ system."""
    
    # Î›Î¯ÏƒÏ„Î± Î¼Îµ test ÎµÏÏ‰Ï„Î®ÏƒÎµÎ¹Ï‚
    test_questions = [
        "What is CloudSphere Platform?",
        "How much does the Professional tier cost?",
        "Do you offer a free trial?",
        "What payment methods do you accept?",
        "How can I reset my password?",
        "What is your refund policy?",
        "Do you support MFA?",
        "What are the API rate limits?",
        "Can I use CloudSphere with Docker?",  # Î•ÏÏÏ„Î·ÏƒÎ· ÎµÎºÏ„ÏŒÏ‚ knowledge base
        "Hello, how are you?"  # Î†ÏƒÏ‡ÎµÏ„Î· ÎµÏÏÏ„Î·ÏƒÎ·
    ]
    
    print("\nğŸ“ Testing FAQ questions...")
    
    for i, question in enumerate(test_questions, 1):
        print(f"\n--- Question {i}/{len(test_questions)} ---")
        print(f"â“ Question: {question}")
        
        # ÎœÎµÏ„ÏÎ¬Î¼Îµ Ï„Î¿Î½ Ï‡ÏÏŒÎ½Î¿ Î±Ï€ÏŒÎºÏÎ¹ÏƒÎ·Ï‚
        start_time = time.time()
        
        # Î£Ï„Î­Î»Î½Î¿Ï…Î¼Îµ Ï„Î·Î½ ÎµÏÏÏ„Î·ÏƒÎ·
        response = requests.post(
            f"{BASE_URL}/api/v1/ask",
            json={"question": question}
        )
        
        end_time = time.time()
        response_time = end_time - start_time
        
        if response.status_code == 200:
            answer = response.json()["answer"]
            print(f"âœ… Answer: {answer[:200]}{'...' if len(answer) > 200 else ''}")
            print(f"â±ï¸  Response time: {response_time:.2f} seconds")
        else:
            print(f"âŒ Error: {response.status_code} - {response.text}")

def test_history():
    """Î¤ÎµÏƒÏ„Î¬ÏÎµÎ¹ Ï„Î¿ history endpoint."""
    print("\nğŸ“š Testing history endpoint...")
    
    response = requests.get(f"{BASE_URL}/api/v1/history?n=5")
    
    if response.status_code == 200:
        history = response.json()
        print(f"âœ… Found {len(history)} questions in history:")
        
        for item in history:
            print(f"\nğŸ• {item['timestamp']}")
            print(f"   Q: {item['question_text'][:100]}...")
            print(f"   A: {item['answer_text'][:100]}...")
    else:
        print(f"âŒ Error: {response.status_code} - {response.text}")

def main():
    """Î¤ÏÎ­Ï‡ÎµÎ¹ ÏŒÎ»Î± Ï„Î± tests."""
    print("ğŸš€ Starting LLM Integration Tests\n")
    
    # Test 1: ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ ÏƒÏÎ½Î´ÎµÏƒÎ·Ï‚
    if not test_llm_connection():
        print("\nâš ï¸  Cannot connect to LLM. Make sure Ollama is running!")
        print("Run: ollama serve")
        return
    
    # Test 2: FAQ ÎµÏÏ‰Ï„Î®ÏƒÎµÎ¹Ï‚
    test_faq_questions()
    
    # Test 3: History
    test_history()
    
    print("\nâœ… All tests completed!")

if __name__ == "__main__":
    main()